{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "This notebook implements a Wikipedia-based chatbot using Python. The chatbot can fetch information from Wikipedia, process the content, and interact with users by answering questions about a given topic. It uses Natural Language Processing (NLP) techniques for text processing and similarity matching to provide relevant answers.\n",
        "\n",
        "\n",
        "####**Technologies Used:**\n",
        "*   Python libraries: requests, BeautifulSoup, nltk, scikit-learn, numpy\n",
        "*   Machine learning techniques: TF-IDF and cosine similarity\n",
        "*   NLP tools: Tokenization, stopword removal, lemmatization"
      ],
      "metadata": {
        "id": "yiW6i1qhA1LH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Installing Required Libraries\n",
        "The following commands install the necessary libraries required for this project:"
      ],
      "metadata": {
        "id": "8imTK_grBiSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4 -q\n",
        "!pip install requests -q\n",
        "!pip install nltk -q\n",
        "!pip install scikit-learn -q\n",
        "!pip install numpy -q"
      ],
      "metadata": {
        "id": "SPXJx2uL1mN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These libraries are essential for web scraping, natural language processing, and text-based machine learning tasks."
      ],
      "metadata": {
        "id": "IDDdW9oyBuf7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importing Libraries\n",
        "This section imports the libraries used in the project.Each library serves the following purposes:\n",
        "\n",
        "*   `requests`: To fetch content from Wikipedia.\n",
        "*   `BeautifulSoup`: For parsing and extracting HTML content.\n",
        "*   `nltk`: For natural language processing tasks such as tokenization and lemmatization.\n",
        "*   `string`: To handle string manipulation.\n",
        "*   `scikit-learn`: For TF-IDF vectorization and similarity calculations.\n",
        "*   `numpy`: For numerical operations.\n",
        "*   `time`: To add delays for a better chatbot interaction experience.\n",
        "*   `logging`: For debugging and tracking the chatbot's internal processes."
      ],
      "metadata": {
        "id": "0p6RGm25B4t2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Required libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from string import punctuation\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from time import sleep\n",
        "import logging"
      ],
      "metadata": {
        "id": "21yjehvl1hA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup and Constants\n",
        "This section initializes logging, downloads required NLTK data, and defines constants used throughout the chatbot:\n",
        "*   `Logging`: Used for tracking the chatbot's operations.\n",
        "*   `NLTK Data`: Downloads essential language processing datasets such as stopwords and lemmatization data.\n",
        "*   `EXIT_COMMANDS`: Words to terminate the chat.\n",
        "*   `THANKS_COMMANDS`: Words for recognizing gratitude.\n",
        "*   `MORE_COMMAND`: Command to request additional details about the topic."
      ],
      "metadata": {
        "id": "eTvtMFEXDDTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Constants for exit commands and special inputs\n",
        "EXIT_COMMANDS = ['bye', 'quit', 'exit']\n",
        "THANKS_COMMANDS = ['thanks', 'thank you', 'thanks for your help', 'thank you very much']\n",
        "MORE_COMMAND = 'more'\n"
      ],
      "metadata": {
        "id": "SBwDrNYW1kYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#WikipediaChatBot Class\n",
        "This class defines the chatbot's functionality. Below are its main components:\n",
        "\n",
        "1.   `__init__`:\n",
        "    *   Initializes the chatbot with necessary variables like terminate_chat, topic_set, and sentences.\n",
        "    *   Displays a welcome message with instructions for the user.\n",
        "2.   `display_welcome_message`:\n",
        "    *   Prints a greeting and explains how to use the chatbot.\n",
        "3.   `start_chat`:\n",
        "    *   Main loop that listens to user input and responds accordingly.\n",
        "    *   Handles special commands like \"bye\", \"more\", and \"thanks\".\n",
        "4.   `fetch_wikipedia_content`:\n",
        "    *   Fetches content from Wikipedia for a given topic.\n",
        "    *   Parses the webpage to extract paragraphs and sentences for processing.\n",
        "5.   `preprocess_text`:\n",
        "    *   Preprocesses text by removing punctuation, stopwords, and applying lemmatization.\n",
        "6.   `handle_user_query`:\n",
        "    *   Responds to user queries by finding the most relevant content using TF-IDF and cosine similarity.\n",
        "7.   `provide_more_info`:\n",
        "    *   Provides additional details about the last discussed topic if requested by the user."
      ],
      "metadata": {
        "id": "kdZtI4-GDzll"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBPtr9pD1Vlk"
      },
      "outputs": [],
      "source": [
        "class WikipediaChatBot:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the chatbot with necessary data and flags.\"\"\"\n",
        "        self.terminate_chat = False\n",
        "        self.topic_set = False\n",
        "        self.topic_title = None\n",
        "        self.paragraphs = []\n",
        "        self.sentences = []\n",
        "        self.current_response_index = None\n",
        "        self.paragraph_indices = []\n",
        "        self.punctuation_removal = str.maketrans('', '', punctuation)\n",
        "        self.lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "        self.stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "        self.display_welcome_message()\n",
        "\n",
        "    def display_welcome_message(self):\n",
        "        \"\"\"Display the initial greeting and instructions.\"\"\"\n",
        "        print(\"Initializing ChatBot...\")\n",
        "        sleep(2)\n",
        "        print('Type \"bye\", \"quit\", or \"exit\" to end the chat.')\n",
        "        sleep(2)\n",
        "        print(\"\\nEnter a topic of interest, and I'll fetch data from Wikipedia.\")\n",
        "        sleep(3)\n",
        "        print('If you want more details after my response, type \"more\".')\n",
        "        sleep(3)\n",
        "        print('-' * 50)\n",
        "\n",
        "    def start_chat(self):\n",
        "        \"\"\"Main chat loop for user interaction.\"\"\"\n",
        "        while not self.terminate_chat:\n",
        "            user_input = input(\"User    >> \").strip().lower()\n",
        "            if user_input in EXIT_COMMANDS:\n",
        "                self.terminate_chat = True\n",
        "                print(\"ChatBot >> Goodbye! Have a great day!\")\n",
        "                sleep(1)\n",
        "            elif user_input == MORE_COMMAND:\n",
        "                self.provide_more_info()\n",
        "            elif user_input in THANKS_COMMANDS:\n",
        "                print(\"ChatBot >> You're welcome!\")\n",
        "            elif not self.topic_set:\n",
        "                self.fetch_wikipedia_content(user_input)\n",
        "            else:\n",
        "                self.handle_user_query(user_input)\n",
        "\n",
        "    def fetch_wikipedia_content(self, topic: str):\n",
        "        \"\"\"Fetch content from Wikipedia for the given topic.\"\"\"\n",
        "        topic = '_'.join(topic.title().split())\n",
        "        url = f'https://en.wikipedia.org/wiki/{topic}'\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            self.topic_title = soup.find('h1').string\n",
        "            self.paragraphs = [p.get_text(strip=True) for p in soup.find_all('p')]\n",
        "            self.sentences = []\n",
        "            self.paragraph_indices = []\n",
        "\n",
        "            for i, paragraph in enumerate(self.paragraphs):\n",
        "                sentences = nltk.sent_tokenize(paragraph)\n",
        "                self.sentences.extend(sentences)\n",
        "                self.paragraph_indices.extend([i] * len(sentences))\n",
        "\n",
        "            self.topic_set = True\n",
        "            print(f\"ChatBot >> Topic set to '{self.topic_title}'. Let's chat!\")\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"ChatBot >> Network error: {e}. Please check your connection.\")\n",
        "        except Exception as e:\n",
        "            print(f\"ChatBot >> Couldn't fetch the topic. Error: {e}. Try a different topic!\")\n",
        "\n",
        "    def preprocess_text(self, text: str) -> list[str]:\n",
        "        \"\"\"Preprocess text by removing punctuation, stopwords, and lemmatizing.\"\"\"\n",
        "        words = nltk.word_tokenize(text.lower().translate(self.punctuation_removal))\n",
        "        return [self.lemmatizer.lemmatize(word) for word in words if word not in self.stopwords]\n",
        "\n",
        "    def handle_user_query(self, query: str):\n",
        "        \"\"\"Handle user queries by finding the most relevant response.\"\"\"\n",
        "        self.sentences.append(query)\n",
        "        vectorizer = TfidfVectorizer(tokenizer=self.preprocess_text)\n",
        "        tfidf_matrix = vectorizer.fit_transform(self.sentences)\n",
        "        similarity_scores = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])\n",
        "        self.sentences.pop()  # Remove user query after vectorization\n",
        "\n",
        "        best_match_index = similarity_scores.argsort()[0, -1]\n",
        "        best_match_score = similarity_scores[0, best_match_index]\n",
        "\n",
        "        if best_match_score > 0:\n",
        "            self.current_response_index = best_match_index\n",
        "            print(f\"ChatBot >> {self.sentences[best_match_index]}\")\n",
        "        else:\n",
        "            print(\"ChatBot >> I'm sorry, I couldn't find relevant information.\")\n",
        "\n",
        "    def provide_more_info(self):\n",
        "        \"\"\"Provide more information about the last discussed topic.\"\"\"\n",
        "        if self.current_response_index is not None:\n",
        "            paragraph_index = self.paragraph_indices[self.current_response_index]\n",
        "            print(f\"ChatBot >> {self.paragraphs[paragraph_index]}\")\n",
        "        else:\n",
        "            print(\"ChatBot >> Please ask a question first!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Running the Bot\n",
        "The chatbot is initialized and started using the following code:"
      ],
      "metadata": {
        "id": "cEJWQgXRF_qC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the chatbot\n",
        "if __name__ == \"__main__\":\n",
        "    bot = WikipediaChatBot()\n",
        "    bot.start_chat()"
      ],
      "metadata": {
        "id": "q0l_M3TI1cFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#How to Use:\n",
        "1.   Enter a topic of interest to fetch information from Wikipedia.\n",
        "2.   Ask questions related to the topic to receive relevant responses.\n",
        "3.   Use the \"more\" command for additional details on the topic.\n",
        "4.   Exit the chat anytime using commands like \"bye\" or \"quit\"."
      ],
      "metadata": {
        "id": "S4JA_MxiGMXt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***`Output`***\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "n6lL-RdN_K1x"
      }
    }
  ]
}